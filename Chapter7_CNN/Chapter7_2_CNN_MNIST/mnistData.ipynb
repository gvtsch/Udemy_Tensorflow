{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten normalisieren\n",
    "Gewichte werden in Abhängigkeit von den Werten angepasst. Im Falle der Farbe Weiß, würde das Gewicht vom Pixelwert 255 oder bei schwarz von 0 angepasst werden. 255 hat dann mehr Gewicht, was ja nicht das Ziel ist, wodurch das Gewicht stärker angepasst würde. Hoher Feature-Wert --> Starke Anpassung. Das ist natürlich NICHT das Ziel.\n",
    "- MinMaxScaling schafft in diesem Beispiel Abhilfe. Die Pixelwerte wertden auf das Intervall [0, 1] \"normalisiert\" ([0, 255] / 255 --> [0, 1])\n",
    "- MinMaxScaling kann aber auch auf das Intervall [-1, 1] gemappt werden: ([0, 255] / 127.5)  - 1 --> [-1, 1]\n",
    "- Es wird immer noch abhängig vom Feature-Wert das Gewicht angepasst, aber mit sehr viel weniger Einfluss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "- Hier werden die vorhandenen Bilder leicht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation und Trainingsdata bzw. Testdata\n",
    "- Die Testdaten werden als allerletztes aufgerufen! Die dienen wirklich nur zum Test. Nicht etwa zum Vergleich von Modellen oder ähnlichem.\n",
    "- Zum Vergleich nutzt man das Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from typing import Tuple\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST:\n",
    "    def __init__(self, with_normalization: bool = True):\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "        self.x_train_: np.ndarray = None\n",
    "        self.y_train_: np.ndarray = None\n",
    "        self.x_vali_: np.ndarray = None\n",
    "        self.y_vali_: np.ndarray = None\n",
    "        self.val_size = 0\n",
    "        self.train_splitted_size = 0\n",
    "        # Preprocess x\n",
    "        self.x_train = x_train.astype(np.float32)\n",
    "        self.x_train = np.expand_dims(x_train, axis=-1)\n",
    "        self.x_test  = x_test.astype(np.float32)\n",
    "        self.x_test = np.expand_dims(x_test, axis=-1)\n",
    "        if with_normalization:\n",
    "            self.x_train = self.x_train / 255.0\n",
    "            self.x_test = self.x_test / 255.0\n",
    "        # Dataset attributes\n",
    "        self.train_size = self.x_train.shape[0]\n",
    "        self.test_size = self.x_test.shape[0]\n",
    "        self.width = self.x_train.shape[1]\n",
    "        self.height = self.x_train.shape[2]\n",
    "        self.depth = self.x_train.shape[3]\n",
    "        self.img_shape = (self.width, self.height, self.depth)\n",
    "        self.num_classes = len(np.unique(y_train))\n",
    "        # Preprocess y\n",
    "        self.y_train = to_categorical(y_train, num_classes=self.num_classes, dtype=np.float32)\n",
    "        self.y_test  = to_categorical(y_test, num_classes=self.num_classes, dtype=np.float32)\n",
    "\n",
    "    def get_train_set(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        return self.x_train, self.y_train\n",
    "    \n",
    "    def get_test_set(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        return self.x_test, self.y_test\n",
    "\n",
    "    def get_splitted_train_validation_set(self, validation_size: float = 0.33) -> Tuple:\n",
    "        self.x_train_, self.x_val_, self.y_train_, self.y_val_ = train_test_split(\n",
    "            self.x_train, \n",
    "            self.y_train, \n",
    "            test_size=validation_size\n",
    "            )\n",
    "        self.val_size = self.x_val_.shape[0]\n",
    "        self.train_splitted_size = self.x_train_.shape[0]\n",
    "        return self.x_train_, self.x_val_, self.y_train_, self.y_val_\n",
    "\n",
    "    def data_augmentation(self, augment_size: int = 5_000) -> None:\n",
    "        image_generator = ImageDataGenerator(\n",
    "            width_shift_range=0.08, # 0.08 -> 8% -> Verschiebung bis zu 2 Pixel \n",
    "            height_shift_range=0.08,\n",
    "            zoom_range=0.05, # %\n",
    "            rotation_range=5\n",
    "        )\n",
    "        # Fit the data generator\n",
    "        image_generator.fit(self.x_train, augment=True)\n",
    "        # Get rnd Train Images for data augmentation\n",
    "        rand_idx = np.random.randint(self.train_size, size=augment_size)\n",
    "        x_augmented = self.x_train[rand_idx].copy()\n",
    "        y_augmented = self.y_train[rand_idx].copy()\n",
    "        x_augmented = image_generator.flow(\n",
    "            x_augmented, \n",
    "            np.zeros(augment_size), \n",
    "            batch_size=augment_size, \n",
    "            shuffle=False).next()[0]\n",
    "        # Append images to train set\n",
    "        self.x_train = np.concatenate((self.x_train, x_augmented))\n",
    "        self.y_train = np.concatenate((self.y_train, y_augmented))\n",
    "        self.train_size = self.x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 60000\n",
      "Test size: 10000\n",
      "Train shape: (60000, 28, 28, 1)\n",
      "Test shape: (10000, 28, 28, 1)\n",
      "Min of x_train: 0.0\n",
      "Max of x_train: 1.0\n"
     ]
    }
   ],
   "source": [
    "data = MNIST()\n",
    "print(f'Train size: {data.train_size}')\n",
    "print(f'Test size: {data.test_size}')\n",
    "print(f'Train shape: {data.x_train.shape}')\n",
    "print(f'Test shape: {data.x_test.shape}')\n",
    "\n",
    "print(f'Min of x_train: {np.min(data.x_train)}')\n",
    "print(f'Max of x_train: {np.max(data.x_train)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "683035455952fa0c5c07396da2eac07ae1d76897e4164c7dcbe8556e60afc848"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('udemy_tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
