{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Klasse mit Keras Hilfsmitteln\n",
    "Funktioniert leider nicht mit dem Keras Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist \n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomRotation # Rotieren eines Bildes (Data Augmentation)\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomTranslation # Verschieben eines Bildes (Data Augmentation)\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomZoom # Zoom Bild (Data Augmentation)\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling # Quasi MinMaxScaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST:\n",
    "    def __init__(self, validation_size: float = 0.33) -> None:\n",
    "        # User-defined constants\n",
    "        self.num_classes = 10\n",
    "        self.batch_size = 128\n",
    "        # Load dataset\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = mnist.load_data()\n",
    "        # Split dataset\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(\n",
    "            self.x_train,\n",
    "            self.y_train,\n",
    "            test_size=validation_size\n",
    "        )\n",
    "        # Preprocess x\n",
    "        self.x_train = np.expand_dims(self.x_train, axis=-1).astype(np.float32)\n",
    "        self.x_test = np.expand_dims(self.x_test, axis=-1).astype(np.float32)\n",
    "        self.x_val = np.expand_dims(self.x_val, axis=-1).astype(np.float32)\n",
    "        # Preprocess y\n",
    "        self.y_train = to_categorical(self.y_train, num_classes=self.num_classes)\n",
    "        self.y_test  = to_categorical(self.y_test, num_classes=self.num_classes)\n",
    "        self.y_val  = to_categorical(self.y_val, num_classes=self.num_classes)\n",
    "        # Dataset attributes\n",
    "        self.train_size = self.x_train.shape[0]\n",
    "        self.test_size = self.x_test.shape[0]\n",
    "        self.tval_size = self.x_val.shape[0]\n",
    "        self.width = self.x_train.shape[1]\n",
    "        self.height = self.x_train.shape[2]\n",
    "        self.depth = self.x_train.shape[3]\n",
    "        self.img_shape = (self.width, self.height, self.depth)\n",
    "        # tf.data.Datasets: Daten aus den Numpy-Arrays holen und ins Dataset packen\n",
    "        self.train_dataset = tf.data.Dataset.from_tensor_slices((self.x_train, self.y_train))\n",
    "        self.test_dataset = tf.data.Dataset.from_tensor_slices((self.x_test, self.y_test))\n",
    "        self.val_dataset = tf.data.Dataset.from_tensor_slices((self.x_val, self.y_val))\n",
    "        # Data Augmentation\n",
    "        self.train_dataset = self._prepare_dataset(self.train_dataset, shuffle=True, augment=True)\n",
    "        self.test_dataset = self._prepare_dataset(self.test_dataset) # Kein Data Augmentation \n",
    "        self.val_dataset = self._prepare_dataset(self.val_dataset) # Kein Data Augmentation \n",
    "\n",
    "    def get_train_set(self) -> tf.data.Dataset:\n",
    "        return self.train_dataset\n",
    "    \n",
    "    def get_test_set(self) -> tf.data.Dataset:\n",
    "        return self.test_dataset\n",
    "\n",
    "    def get_val_set(self) -> tf.data.Dataset:\n",
    "        return self.val_dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_preprocessing() -> Sequential:\n",
    "        model = Sequential()\n",
    "        model.add(Rescaling(scale=(1./255.), offset=0.0))\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def _build_data_augmentation() -> Sequential:\n",
    "        model = Sequential()\n",
    "        model.add(RandomRotation(factor=0.08))\n",
    "        model.add(RandomTranslation(height_factor=0.08, width_factor=0.08))\n",
    "        model.add(RandomZoom(height_factor=0.08, width_factor=0.08))\n",
    "        return model\n",
    "\n",
    "    def _prepare_dataset(\n",
    "        self,\n",
    "        dataset: tf.data.Dataset,\n",
    "        shuffle: bool = False,\n",
    "        augment: bool = False\n",
    "    ) -> tf.data.Dataset:\n",
    "        preprocessing_model = self._build_preprocessing()\n",
    "        dataset = dataset.map(\n",
    "            map_func=lambda x, y: (preprocessing_model(x, training=False), y),\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=1_000)\n",
    "        dataset = dataset.batch(batch_size=self.batch_size)\n",
    "        if augment:\n",
    "            data_augmentation_model = self._build_data_augmentation()\n",
    "            dataset = dataset.map(\n",
    "                map_func=lambda x, y: (data_augmentation_model(x, training=False), y),\n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "            )\n",
    "        return dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "            \n",
    "\n",
    "    # staticmethods geh√∂ren zur Klasse, verwenden aber keine Self-Attribute\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MNIST' object has no attribute 'val_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37072/2485612279.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37072/1721241023.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, validation_size)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Kein Data Augmentation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Kein Data Augmentation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_train_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MNIST' object has no attribute 'val_dataset'"
     ]
    }
   ],
   "source": [
    "data = MNIST()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "683035455952fa0c5c07396da2eac07ae1d76897e4164c7dcbe8556e60afc848"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('udemy_tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
