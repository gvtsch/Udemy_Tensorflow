{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Skript wird nicht auf ein sequentielles Model (Sequential) gesetzt, sondern auf ein Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Tuple\n",
    "\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler, TensorBoard\n",
    "from tensorflow.keras.layers import ReLU\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "from tf_utils.mnistData_advance import MNIST\n",
    "\n",
    "from tf_utils.callbacks import schedule_fn, schedule_fn2, schedule_fn3, schedule_fn4, LRTensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS_DIR = os.path.abspath('C:/Selbststudium/Udemy/Udemy_Tensorflow/logs/MNIST/')\n",
    "if not os.path.exists(LOGS_DIR):\n",
    "    os.mkdir(LOGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    img_shape: Tuple[int, int, int],\n",
    "    num_classes: int,\n",
    "    optimizer: tf.keras.optimizers.Optimizer,\n",
    "    learning_rate: float,\n",
    "    filter_block_1: int,\n",
    "    kernel_size_block_1: int,\n",
    "    filter_block_2: int,\n",
    "    kernel_size_block_2: int,\n",
    "    filter_block_3: int,\n",
    "    kernel_size_block_3: int,\n",
    "    dense_layer_size: int,\n",
    "    kernel_initializer: tf.keras.initializers.Initializer,\n",
    "    activation_cls: tf.keras.layers.Activation,\n",
    "    dropout_rate: float,\n",
    "    use_batch_normalization: bool\n",
    ") -> Model:\n",
    "    input_img = Input(shape=img_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=filter_block_1, \n",
    "        kernel_size=kernel_size_block_1, \n",
    "        padding='same', \n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(input_img)\n",
    "    if use_batch_normalization:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = activation_cls(x)\n",
    "    x = Conv2D(\n",
    "        filters=filter_block_1, \n",
    "        kernel_size=kernel_size_block_1, \n",
    "        padding='same', \n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(x)\n",
    "    if use_batch_normalization:\n",
    "        x = BatchNormalization()(x)\n",
    "    if dropout_rate:\n",
    "        x = Dropout(rate=dropout_rate)(x)\n",
    "    x = activation_cls(x)\n",
    "    x = MaxPool2D()(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=filter_block_2, \n",
    "        kernel_size=kernel_size_block_2, \n",
    "        padding='same', \n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(x)\n",
    "    if use_batch_normalization:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = activation_cls(x)\n",
    "    x = Conv2D(\n",
    "        filters=filter_block_2, \n",
    "        kernel_size=kernel_size_block_2, \n",
    "        padding='same', \n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(x)\n",
    "    if use_batch_normalization:\n",
    "        x = BatchNormalization()(x)\n",
    "    if dropout_rate:\n",
    "        x = Dropout(rate=dropout_rate)(x)\n",
    "    x = activation_cls(x)\n",
    "    x = MaxPool2D()(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=filter_block_3, \n",
    "        kernel_size=kernel_size_block_3, \n",
    "        padding='same', \n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(x)\n",
    "    if use_batch_normalization:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = activation_cls(x)\n",
    "    x = Conv2D(\n",
    "        filters=filter_block_3, \n",
    "        kernel_size=kernel_size_block_3, \n",
    "        padding='same', \n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(x)\n",
    "    if use_batch_normalization:\n",
    "        x = BatchNormalization()(x)\n",
    "    if dropout_rate:\n",
    "        x = Dropout(rate=dropout_rate)(x)\n",
    "    x = activation_cls(x)\n",
    "    x = MaxPool2D()(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(\n",
    "        units=dense_layer_size, \n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(x)\n",
    "    if use_batch_normalization:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = activation_cls(x)\n",
    "    x = Dense(\n",
    "        units=num_classes, \n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(x)\n",
    "    y_pred = Activation('softmax')(x)\n",
    "\n",
    "    # Jetzt muss noch ein Modell Objekt mit eben obiger Struktur erstellt werden\n",
    "    model = Model(\n",
    "        inputs = [input_img],\n",
    "        outputs = [y_pred]\n",
    "    )\n",
    "    \n",
    "    opt = optimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', # wird bei Kategorie-Problemen mit mehr als 2 Klassen genommen\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "# Best model params\n",
    "params = {\n",
    "    \"optimizer\": Adam,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"filter_block_1\": 32,\n",
    "    \"kernel_size_block_1\": 3,\n",
    "    \"filter_block_2\": 64,\n",
    "    \"kernel_size_block_2\": 3,\n",
    "    \"filter_block_3\": 128,\n",
    "    \"kernel_size_block_3\": 3,\n",
    "    \"dense_layer_size\": 128,\n",
    "    \"kernel_initializer\": \"GlorotUniform\",\n",
    "    \"activation_cls\": ReLU(),\n",
    "    \"dropout_rate\": 0.0,\n",
    "    \"use_batch_normalization\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 1/100\n",
      "315/315 [==============================] - 8s 13ms/step - loss: 0.1060 - accuracy: 0.9709 - val_loss: 2.3124 - val_accuracy: 0.3438 - lr: 0.0010 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/100\n",
      "315/315 [==============================] - 4s 13ms/step - loss: 0.0319 - accuracy: 0.9905 - val_loss: 0.1389 - val_accuracy: 0.9578 - lr: 0.0010 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 0.0192 - accuracy: 0.9949 - val_loss: 0.0486 - val_accuracy: 0.9864 - lr: 0.0010 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/100\n",
      "315/315 [==============================] - 4s 11ms/step - loss: 0.0149 - accuracy: 0.9956 - val_loss: 0.0816 - val_accuracy: 0.9753 - lr: 0.0010 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 0.0209 - accuracy: 0.9934 - val_loss: 0.0387 - val_accuracy: 0.9888 - lr: 0.0010 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.0415 - val_accuracy: 0.9885 - lr: 0.0010 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 0.0092 - accuracy: 0.9972 - val_loss: 0.0629 - val_accuracy: 0.9823 - lr: 0.0010 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 8/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 0.0172 - accuracy: 0.9946 - val_loss: 0.0328 - val_accuracy: 0.9896 - lr: 0.0010 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 9/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 0.0053 - accuracy: 0.9987 - val_loss: 0.0390 - val_accuracy: 0.9895 - lr: 0.0010 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 10/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0295 - val_accuracy: 0.9917 - lr: 0.0010 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 11/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0229 - val_accuracy: 0.9935 - lr: 0.0010 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.0009048374180359595.\n",
      "Epoch 12/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0432 - val_accuracy: 0.9883 - lr: 9.0484e-04 - learning_rate: 9.0484e-04\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.0008187307530779819.\n",
      "Epoch 13/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.0331 - val_accuracy: 0.9906 - lr: 8.1873e-04 - learning_rate: 8.1873e-04\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.0007408182206817179.\n",
      "Epoch 14/100\n",
      "315/315 [==============================] - 4s 13ms/step - loss: 0.0108 - accuracy: 0.9966 - val_loss: 0.0284 - val_accuracy: 0.9925 - lr: 7.4082e-04 - learning_rate: 7.4082e-04\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.0006703200460356394.\n",
      "Epoch 15/100\n",
      "315/315 [==============================] - 4s 14ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0250 - val_accuracy: 0.9934 - lr: 6.7032e-04 - learning_rate: 6.7032e-04\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.0006065306597126335.\n",
      "Epoch 16/100\n",
      "315/315 [==============================] - 4s 13ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0264 - val_accuracy: 0.9925 - lr: 6.0653e-04 - learning_rate: 6.0653e-04\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.0005488116360940264.\n",
      "Epoch 17/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 0.0055 - accuracy: 0.9984 - val_loss: 0.0665 - val_accuracy: 0.9826 - lr: 5.4881e-04 - learning_rate: 5.4881e-04\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004965853037914095.\n",
      "Epoch 18/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0238 - val_accuracy: 0.9937 - lr: 4.9659e-04 - learning_rate: 4.9659e-04\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004493289641172216.\n",
      "Epoch 19/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 9.2443e-04 - accuracy: 0.9999 - val_loss: 0.0217 - val_accuracy: 0.9943 - lr: 4.4933e-04 - learning_rate: 4.4933e-04\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.00040656965974059914.\n",
      "Epoch 20/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 4.7698e-04 - accuracy: 1.0000 - val_loss: 0.0217 - val_accuracy: 0.9947 - lr: 4.0657e-04 - learning_rate: 4.0657e-04\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.00036787944117144236.\n",
      "Epoch 21/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 5.4299e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9941 - lr: 3.6788e-04 - learning_rate: 3.6788e-04\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.00033287108369807955.\n",
      "Epoch 22/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 3.1239e-04 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 0.9948 - lr: 3.3287e-04 - learning_rate: 3.3287e-04\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.00030119421191220205.\n",
      "Epoch 23/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 2.1524e-04 - accuracy: 1.0000 - val_loss: 0.0218 - val_accuracy: 0.9948 - lr: 3.0119e-04 - learning_rate: 3.0119e-04\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.0002725317930340126.\n",
      "Epoch 24/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 1.7532e-04 - accuracy: 1.0000 - val_loss: 0.0218 - val_accuracy: 0.9946 - lr: 2.7253e-04 - learning_rate: 2.7253e-04\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.00024659696394160646.\n",
      "Epoch 25/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 1.3628e-04 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 0.9947 - lr: 2.4660e-04 - learning_rate: 2.4660e-04\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.00022313016014842982.\n",
      "Epoch 26/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 1.2084e-04 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9948 - lr: 2.2313e-04 - learning_rate: 2.2313e-04\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.00020189651799465538.\n",
      "Epoch 27/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 1.7805e-04 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9947 - lr: 2.0190e-04 - learning_rate: 2.0190e-04\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.0001826835240527346.\n",
      "Epoch 28/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0227 - val_accuracy: 0.9947 - lr: 1.8268e-04 - learning_rate: 1.8268e-04\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.00016529888822158653.\n",
      "Epoch 29/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 7.3130e-04 - accuracy: 0.9999 - val_loss: 0.0223 - val_accuracy: 0.9946 - lr: 1.6530e-04 - learning_rate: 1.6530e-04\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.00014956861922263504.\n",
      "Epoch 30/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 5.3713e-04 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9943 - lr: 1.4957e-04 - learning_rate: 1.4957e-04\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.0001353352832366127.\n",
      "Epoch 31/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 1.9292e-04 - accuracy: 1.0000 - val_loss: 0.0218 - val_accuracy: 0.9943 - lr: 1.3534e-04 - learning_rate: 1.3534e-04\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.0001224564282529819.\n",
      "Epoch 32/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 1.5641e-04 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9943 - lr: 1.2246e-04 - learning_rate: 1.2246e-04\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.00011080315836233387.\n",
      "Epoch 33/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 1.3665e-04 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9944 - lr: 1.1080e-04 - learning_rate: 1.1080e-04\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.00010025884372280371.\n",
      "Epoch 34/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 1.2409e-04 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9943 - lr: 1.0026e-04 - learning_rate: 1.0026e-04\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 9.071795328941248e-05.\n",
      "Epoch 35/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 1.4037e-04 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9945 - lr: 9.0718e-05 - learning_rate: 9.0718e-05\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 8.20849986238988e-05.\n",
      "Epoch 36/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 1.1586e-04 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9945 - lr: 8.2085e-05 - learning_rate: 8.2085e-05\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 7.427357821433387e-05.\n",
      "Epoch 37/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 9.4638e-05 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 0.9947 - lr: 7.4274e-05 - learning_rate: 7.4274e-05\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 6.720551273974975e-05.\n",
      "Epoch 38/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 8.4402e-05 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9946 - lr: 6.7206e-05 - learning_rate: 6.7206e-05\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 6.0810062625217954e-05.\n",
      "Epoch 39/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 8.1638e-05 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9947 - lr: 6.0810e-05 - learning_rate: 6.0810e-05\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 5.502322005640721e-05.\n",
      "Epoch 40/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 7.8387e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9947 - lr: 5.5023e-05 - learning_rate: 5.5023e-05\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 4.9787068367863945e-05.\n",
      "Epoch 41/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 6.9543e-05 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9947 - lr: 4.9787e-05 - learning_rate: 4.9787e-05\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 4.50492023935578e-05.\n",
      "Epoch 42/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 7.8078e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9948 - lr: 4.5049e-05 - learning_rate: 4.5049e-05\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 4.0762203978366214e-05.\n",
      "Epoch 43/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 6.8888e-05 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9947 - lr: 4.0762e-05 - learning_rate: 4.0762e-05\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 3.688316740124e-05.\n",
      "Epoch 44/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 6.4460e-05 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9947 - lr: 3.6883e-05 - learning_rate: 3.6883e-05\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 3.337326996032607e-05.\n",
      "Epoch 45/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 5.6049e-05 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9947 - lr: 3.3373e-05 - learning_rate: 3.3373e-05\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 3.01973834223185e-05.\n",
      "Epoch 46/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 5.0709e-05 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9946 - lr: 3.0197e-05 - learning_rate: 3.0197e-05\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 2.732372244729256e-05.\n",
      "Epoch 47/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 5.1156e-05 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9946 - lr: 2.7324e-05 - learning_rate: 2.7324e-05\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 2.472352647033939e-05.\n",
      "Epoch 48/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 4.8586e-05 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9947 - lr: 2.4724e-05 - learning_rate: 2.4724e-05\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 2.2370771856165592e-05.\n",
      "Epoch 49/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 4.5145e-05 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9947 - lr: 2.2371e-05 - learning_rate: 2.2371e-05\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 2.024191144580438e-05.\n",
      "Epoch 50/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 4.0949e-05 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9947 - lr: 2.0242e-05 - learning_rate: 2.0242e-05\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 1.831563888873418e-05.\n",
      "Epoch 51/100\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 4.3052e-05 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9949 - lr: 1.8316e-05 - learning_rate: 1.8316e-05\n",
      "\n",
      "Epoch 00052: LearningRateScheduler setting learning rate to 1.657267540176124e-05.\n",
      "Epoch 52/100\n",
      "314/315 [============================>.] - ETA: 0s - loss: 3.6452e-05 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 22.\n",
      "315/315 [==============================] - 4s 12ms/step - loss: 3.6920e-05 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9948 - lr: 1.6573e-05 - learning_rate: 1.6573e-05\n",
      "Epoch 00052: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d91b0cb040>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = MNIST()\n",
    "\n",
    "train_dataset = data.get_train_set()\n",
    "val_dataset = data.get_val_set()\n",
    "test_dataset = data.get_test_set()\n",
    "\n",
    "img_shape = data.img_shape\n",
    "num_classes = data.num_classes\n",
    "\n",
    "schedules = [schedule_fn, schedule_fn2, schedule_fn3, schedule_fn4]\n",
    "\n",
    "model_log_dir = os.path.join(LOGS_DIR, f\"mnist_improve\")\n",
    "\n",
    "model = build_model(\n",
    "    img_shape,\n",
    "    num_classes,\n",
    "    **params\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', # wird bei Kategorie-Problemen mit mehr als 2 Klassen genommen\n",
    "    optimizer=Adam(learning_rate=0.0005),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "lr_callback = LRTensorBoard(\n",
    "    log_dir=model_log_dir,\n",
    "    histogram_freq=0,\n",
    "    profile_batch=0,\n",
    "    write_graph=0\n",
    ")\n",
    "\n",
    "lrs_callback = LearningRateScheduler(\n",
    "    schedule=schedule_fn2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plateau_callback = ReduceLROnPlateau(\n",
    "    monitor=\"val_accuracy\",\n",
    "    factor=0.99,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_lr=1e-5\n",
    ")\n",
    "\n",
    "es_callback = EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=30,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    "    min_delta=0.0005\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    verbose=1,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[es_callback, lrs_callback, lr_callback],\n",
    "    validation_data=val_dataset,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "683035455952fa0c5c07396da2eac07ae1d76897e4164c7dcbe8556e60afc848"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('udemy_tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
