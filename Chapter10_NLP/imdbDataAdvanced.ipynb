{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Dataset (NLP)\n",
    "IMDB ist beliebtestes Dataset und besteht aus 25_000 Film-Reviews fürs Training und 25_000 fürs Testing. Review ist entweder positiv oder negativ. Netzwerk soll erkennen, ob der Text einer Review positiv oder negativ ist --> Text Klassifikation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import TextVectorization \n",
    "    # Transformiert Wörter in Integers. Jedes Wort wird durch einen Index ersetzt (gleiche Wörter -> gleicher Index)\n",
    "    # Per Default alles zu lowercase\n",
    "    # ... --> https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorcross.utils import dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class IMDB:\n",
    "    def __init__(self, vocab_size: int, sequence_length: int, validation_size: float = 0.33) -> None:\n",
    "        # User-defined constants\n",
    "        self.num_classes = 2\n",
    "        self.batch_size = 128\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        # Load dataset\n",
    "        dataset = tfds.load('imdb_reviews', as_supervised=True)\n",
    "        self.train_dataset, self.val_dataset = dataset_split(dataset['train'], split_fraction=validation_size)\n",
    "        self.test_dataset = dataset['test']\n",
    "        # Dataset attributes\n",
    "        self.train_size = len(self.train_dataset)\n",
    "        self.test_size = len(self.test_dataset)\n",
    "        self.tval_size = len(self.val_dataset)\n",
    "        # Vectorization layer\n",
    "        self.vectorization_layer = TextVectorization(\n",
    "            max_tokens=self.vocab_size,\n",
    "            output_mode='int',\n",
    "            output_sequence_length=self.sequence_length\n",
    "        )\n",
    "        text_data = self.train_dataset.map(lambda x,y: x) # Nur Wörter und nicht etwa die Klasse übergeben\n",
    "        self.vectorization_layer.adapt(text_data) # Schaut sich das Dataset an, welche Wörter es gibt und bestimmt damit das Vokabular\n",
    "        self.vocabulary = self.vectorization_layer.get_vocabulary()\n",
    "        self.word_index = dict(zip(self.vocabulary, range(len(self.vocabulary))))\n",
    "        # Prepare Datasets\n",
    "        self.train_dataset = self._prepare_dataset(self.train_dataset)\n",
    "        self.test_dataset = self._prepare_dataset(self.test_dataset) \n",
    "        self.val_dataset = self._prepare_dataset(self.val_dataset) \n",
    "\n",
    "    def get_train_set(self) -> tf.data.Dataset:\n",
    "        return self.train_dataset\n",
    "    \n",
    "    def get_test_set(self) -> tf.data.Dataset:\n",
    "        return self.test_dataset\n",
    "\n",
    "    def get_val_set(self) -> tf.data.Dataset:\n",
    "        return self.val_dataset\n",
    "\n",
    "    def _build_preprocessing(self) -> Sequential:\n",
    "        model = Sequential()\n",
    "        model.add(self.vectorization_layer)\n",
    "        return model\n",
    "\n",
    "    def _mask_to_categorical(self, x: tf.Tensor, y: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        y = tf.one_hot(tf.cast(y, tf.int32), depth=self.num_classes)\n",
    "        y = tf.cast(y, tf.float32)\n",
    "        return x, y\n",
    "\n",
    "    def _prepare_dataset(\n",
    "        self,\n",
    "        dataset: tf.data.Dataset,\n",
    "        shuffle: bool = False,\n",
    "        augment: bool = False\n",
    "    ) -> tf.data.Dataset:\n",
    "        preprocessing_model = self._build_preprocessing()\n",
    "        dataset = dataset.map(\n",
    "            map_func=lambda x, y: (preprocessing_model(x, training=False), y),\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "        dataset = dataset.map(\n",
    "            map_func=lambda x, y: self._mask_to_categorical(x, y),\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "\n",
    "        dataset = dataset.batch(batch_size=self.batch_size)\n",
    "\n",
    "        return dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20_000\n",
    "sequence_length = 80\n",
    "imdb_data = IMDB(vocab_size, sequence_length)\n",
    "train_dataset = imdb_data.get_train_set()\n",
    "# print(imdb_data.vocabulary)\n",
    "# print(imdb_data.word_index)\n",
    "for text_batch, label_batch in train_dataset.take(1):\n",
    "    for i in range(3):    \n",
    "        print(text_batch[i].numpy(), label_batch[i].numpy())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "683035455952fa0c5c07396da2eac07ae1d76897e4164c7dcbe8556e60afc848"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('udemy_tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
